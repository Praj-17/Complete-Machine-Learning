{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2f5cFp7Yyxm"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r5ydHrfaUs2p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rne-cyCLaGBI"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oigc4G3Va8FV",
        "outputId": "98673429-62d5-4886-b4bd-72849242cb3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 31 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   mean radius              569 non-null    float64\n",
            " 1   mean texture             569 non-null    float64\n",
            " 2   mean perimeter           569 non-null    float64\n",
            " 3   mean area                569 non-null    float64\n",
            " 4   mean smoothness          569 non-null    float64\n",
            " 5   mean compactness         569 non-null    float64\n",
            " 6   mean concavity           569 non-null    float64\n",
            " 7   mean concave points      569 non-null    float64\n",
            " 8   mean symmetry            569 non-null    float64\n",
            " 9   mean fractal dimension   569 non-null    float64\n",
            " 10  radius error             569 non-null    float64\n",
            " 11  texture error            569 non-null    float64\n",
            " 12  perimeter error          569 non-null    float64\n",
            " 13  area error               569 non-null    float64\n",
            " 14  smoothness error         569 non-null    float64\n",
            " 15  compactness error        569 non-null    float64\n",
            " 16  concavity error          569 non-null    float64\n",
            " 17  concave points error     569 non-null    float64\n",
            " 18  symmetry error           569 non-null    float64\n",
            " 19  fractal dimension error  569 non-null    float64\n",
            " 20  worst radius             569 non-null    float64\n",
            " 21  worst texture            569 non-null    float64\n",
            " 22  worst perimeter          569 non-null    float64\n",
            " 23  worst area               569 non-null    float64\n",
            " 24  worst smoothness         569 non-null    float64\n",
            " 25  worst compactness        569 non-null    float64\n",
            " 26  worst concavity          569 non-null    float64\n",
            " 27  worst concave points     569 non-null    float64\n",
            " 28  worst symmetry           569 non-null    float64\n",
            " 29  worst fractal dimension  569 non-null    float64\n",
            " 30  target                   569 non-null    int32  \n",
            "dtypes: float64(30), int32(1)\n",
            "memory usage: 135.7 KB\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
        "df['target'] = data['target']\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6A346EOkWfB"
      },
      "source": [
        "All our features are numerical, so we don't need to convert any of them. Also, we have no null values since there are 569 total entries and all features have 569 non-null entries, so we don't need to do any imputation. This is a pretty clean data set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwiWkrAjUrq",
        "outputId": "35b795a9-ad72-4252-9c4c-5e20b5e7708a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    357\n",
              "0    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN5Y30Rnj27U"
      },
      "source": [
        "Data is slightly unbalanced, but for this exercise, it's okay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD89VLoHvCoH"
      },
      "source": [
        "##Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zfYhzYfJY6b-"
      },
      "outputs": [],
      "source": [
        "# assign inputs and output\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gOhd1xvgZkmJ"
      },
      "outputs": [],
      "source": [
        "# randomly split data into train and test dataframes.\n",
        "# 30% of the data will be in the test dataframe and 70% will go into the train set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC2DrK1xaGym"
      },
      "source": [
        "## Train & Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j8WZAO0OWY5z"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=3000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2K1pzpaosSS"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJOhqXAna7hz",
        "outputId": "cab49515-f31e-4d00-8a0c-f44903f6f193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on test set:  0.9415204678362573\n",
            "\n",
            "confusion matrix:\n",
            "\n",
            " [[ 59   3]\n",
            " [  7 102]]\n",
            "\n",
            "classification report\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92        62\n",
            "           1       0.97      0.94      0.95       109\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.94      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "print('accuracy on test set: ', accuracy_score(y_test, predictions))\n",
        "print('\\nconfusion matrix:\\n\\n', confusion_matrix(predictions, y_test))\n",
        "print('\\nclassification report\\n\\n', classification_report(predictions, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "12 - Logistic Regression.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1fb457075518da48b6f10d6a89d1c79a5d3a8a4cce5ab48738750cf0a5c90cbc"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
